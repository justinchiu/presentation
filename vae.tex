\documentclass{beamer}
% \mode<presentation>
\setbeamertemplate{navigation symbols}{}
\let\tempone\itemize
\let\temptwo\enditemize
\renewenvironment{itemize}{\tempone\addtolength{\itemsep}{0.5\baselineskip}}{\temptwo}
\usepackage{beamerthemeshadow}
\usepackage[normalem]{ulem}
\usepackage{tikz}
\usepackage{tikz-dependency}
\usetikzlibrary{shapes.arrows}

\tikzset{
    myarrow/.style={
        draw,
        fill=gray,
        single arrow,
        minimum height=3.5ex,
        single arrow head extend=1ex
    }
}
\newcommand{\arrowup}{%
\tikz [baseline=-0.5ex]{\node [myarrow,rotate=90] {};}
}
\newcommand{\arrowdown}{%
\tikz [baseline=-1ex]{\node [myarrow,rotate=-90] {};}
}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{pgffor}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{tabularx}
\usepackage{tikz,etoolbox}
\usepackage{tikz,amsmath,siunitx}
\usetikzlibrary{arrows,snakes,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}

\usepackage{subcaption}
% \usepackage{url}
% \usepackage{hyperref}
\usepackage{pgf}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage[absolute,overlay]{textpos}
\usetikzlibrary{shapes,arrows,positioning,automata,positioning,spy,matrix,scopes,chains}
\newcommand{\digs}[2]{\hphantom{999}\llap{#1}\,+\,\hphantom{999}\llap{#2}}
\setbeamersize{text margin left=6mm}
\setbeamersize{text margin right=6mm}
\renewcommand{\insertnavigation}[1]{}
\setbeamertemplate{headline}{}
\setbeamertemplate{footline}{}
\usefonttheme{professionalfonts}
\setbeamercovered{transparent}
\mode<presentation>
\linespread{1.25}
\DeclareMathOperator{\Tr}{Tr} 

\usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[all,dvips]{xy}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{framed}
\usepackage{natbib}
\usepackage[labelformat=empty]{caption}
\newcommand{\air}{\vspace{0.25cm}}
\newcommand{\mair}{\vspace{-0.25cm}}

\setbeamertemplate{navigation symbols}{}%remove navigation symbols
\renewcommand{\rmdefault}{crm}
\newcommand{\lnbrack}{{\normalfont [}}
\newcommand{\rnbrack}{{\normalfont ]}\thinspace}
\newcommand{\lbbrack}{\textcolor{red}{\textbf{[}}}
\newcommand{\rbbrack}{\textcolor{red}{\textbf{]}}\thinspace}
\definecolor{vermillion}{RGB}{213,94,0}
\newcommand{\given}{\,|\,}
\definecolor{orange}{RGB}{230,159,0}
\definecolor{skyblue}{RGB}{86,180,233}
\definecolor{bluegreen}{RGB}{90,143,41}
% \definecolor{bluegreen}{RGB}{0,158,115}
\definecolor{myyellow}{RGB}{240,228,66} % i dunno if this is the same as standard yellow
\definecolor{myblue}{RGB}{0,114,178}
\definecolor{vermillion}{RGB}{213,94,0}
\definecolor{redpurple}{RGB}{204,121,167}
\definecolor{lightgrey}{RGB}{234,234,234}

\newcommand{\ha}{\boldh_{\ua}}
\newcommand{\hp}{\boldh_{\up}}
\newcommand{\pgrad}{\nabla_{p}^\mathcal{L}}

\newcommand{\todoy}[1]{\textcolor{red}{Fix me (yoon): #1}}
\newcommand{\todos}[1]{\textcolor{red}{Fix me (sasha): #1}}

\newcommand{\hc}{\boldh_{\mathrm{c}}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\logadd}{logadd}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\signexp}{signexp}
\DeclareMathOperator{\sigmoid}{sigmoid}
\DeclareMathOperator{\fwdbwd}{ForwardBackward}
\DeclareMathOperator{\softparent}{soft-parent}
\DeclareMathOperator{\parent}{parent}
\DeclareMathOperator{\head}{head}
\DeclareMathOperator{\softhead}{soft-head}
\DeclareMathOperator{\simf}{sim}
\DeclareMathOperator{\NN}{NNet}
\DeclareMathOperator{\attn}{Attn}
\DeclareMathOperator{\relu}{ReLU}
\DeclareMathOperator{\lstm}{LSTM}
\DeclareMathOperator{\rnn}{RNN}
\DeclareMathOperator{\mlp}{MLP}



\usetikzlibrary{positioning}
% \setbeamerfont{alerted text}{series=\bfseries}
% \setbeamerfont{structure}{series=\bfseries}
% Needed for diakgrams.
\def\im#1#2{
  \node(#1) [scale=#2]{\pgfbox[center,top]{\pgfuseimage{#1}}
};}
% \input{pictures_header}


\title[latent]{Semi-supervised Learning with Deep Generative Models}

\author[Demi, Justin, Yuntian]{March 20, 2018}
\institute[Harvard SEAS]{ 
{ }

 % Code: \textbf{https://github.com/harvardnlp/struct-attn}}
% \vspace{5mm}

% \hspace{-70mm} $^*$Equal Contribution
}
\date{}
% \usetheme{Madrid}
\definecolor{darkgreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{darkpurple}{rgb}{0.55, 0.0, 0.55}

\newcommand{\enc}{\mathrm{src}}

\newcommand{\yvec}{\mathbf{y}}
\newcommand{\tvec}{\mathbf{t}}
\newcommand{\wvec}{\mathbf{w}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\zvec}{\mathbf{z}}
\newcommand{\E}{\mathbbm{E}}

\newcommand{\mcD}{\mathcal{D}}

% \newcommand{\mcY}{\mathcal{Y}}
% \newcommand{\mcV}{\mathcal{V}}
\newcommand{\context}{\mathbf{w}_{\mathrm{c}}}
\newcommand{\embcontext}{\mathbf{\tilde{w}}_{\mathrm{c}}}
\newcommand{\inpcontext}{\mathbf{\tilde{x}}}
\newcommand{\start}{\mathbf{\tilde{y}}_{\mathrm{c0}}}
\newcommand{\End}{\mathrm{\texttt{</s>}}}

\newcommand{\Uvec}{\mathbf{U}}
\newcommand{\Evec}{\mathbf{E}}
\newcommand{\Gvec}{\mathbf{G}}
\newcommand{\Fvec}{\mathbf{F}}
\newcommand{\Pvec}{\mathbf{P}}
\newcommand{\pvec}{\mathbf{p}}
\newcommand{\q}{\mathbf{Q}}
\newcommand{\Vvec}{\mathbf{V}}
\newcommand{\Wvec}{\mathbf{W}}
\newcommand{\h}{\mathbf{h}}
% \newcommand{\reals}{\mathbb{R}}

\newcommand{\Cite}[1]{{\footnotesize \citep{#1}}}
\newcommand{\TT}[1]{{\footnotesize\tt{#1}}}
\newcommand{\roplus}{{\color{red} \bigoplus}}
\newcommand{\rotimes}{{\color{red} \,\otimes\,}}

\newcommand{\boldw}{\boldsymbol{w}}
\newcommand{\xvec}{\mathbf{x}}

\newcommand{\boldu}{\boldsymbol{u}}
\newcommand{\boldv}{\boldsymbol{v}}
\newcommand{\boldb}{\boldsymbol{b}}
\newcommand{\boldW}{\boldsymbol{W}}
\newcommand{\boldh}{\boldsymbol{h}}
\newcommand{\boldg}{\boldsymbol{g}}
\newcommand{\ua}{\ensuremath{\mathrm{a}}}
\newcommand{\up}{\ensuremath{\mathrm{p}}}
%\newcommand{\bphi}{\ensuremath{\mathbf{\phi}}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\mcY}{\mathcal{Y}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\mcC}{\mathcal{C}}
\newcommand{\mcA}{\mathcal{A}}
\newcommand{\mcV}{\mathcal{V}}
\newcommand{\trans}{\ensuremath{\mathsf{T}}}
\def\argmin{\operatornamewithlimits{arg\,min}}
\def\argmax{\operatornamewithlimits{arg\,max}}
\newcommand{\reals}{\ensuremath{\mathbb{R}}}

\newcommand{\aphi}{\boldsymbol{\phi}_{\mathrm{a}}}
\newcommand{\pwphi}{\boldsymbol{\phi}_{\mathrm{p}}}
\newcommand{\squigaphi}{\widetilde{\boldsymbol{\phi}}_{\mathrm{a}}}
\newcommand{\squigpwphi}{\widetilde{\boldsymbol{\phi}}_{\mathrm{p}}}

\newcommand{\aW}{\boldW_{\mathrm{\ua}}}
\newcommand{\pW}{\boldW_{\mathrm{\up}}}

\newcommand{\ab}{\boldb_{\mathrm{\ua}}}
\newcommand{\pb}{\boldb_{\mathrm{\up}}}

\newcommand{\Da}{d_{\mathrm{a}}}
\newcommand{\Dp}{d_{\mathrm{p}}}

% \newcommand{\ha}{\boldh_{\ua}}
% \newcommand{\hp}{\boldh_{\up}}

\newcommand{\ourmodel}{This work}
\newcommand{\zro}{{\color{white}0}}
\AtBeginSection[]
{
  \begin{frame}
  \tableofcontents[currentsection]
  \end{frame}
}

\AtBeginSubsection[]
{
  \begin{frame}
  \tableofcontents[currentsubsection]
  \end{frame}
}


\def\argmax{\operatornamewithlimits{arg\,max}}
\def\kargmax{\operatornamewithlimits{K-arg\,max}}
\setbeamercovered{transparent}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}
  \begin{center}
    \structure{Background}
    \begin{itemize}
      \item Paper: Semi-supervised Learning with Deep Generative Models
      \item \textbf{Semi-supervised} learning considers the problem of classification when only a small subset of the 
      observations have corresponding class labels.
      \item Main contributions:
        \begin{enumerate}
          \item New framework for semi-supervised learning with generative models
          \item First time bring variational inference to bear upon the problem of semi-supervised classification
          \item Better performance on several benchmark problems
          \item Generative semi-supervised models learn to separate the data classes (content types) from the intra-class variabilities (styles)
        \end{enumerate}
      \end{itemize}
   \end{center} 
\end{frame}

\section{Latent-feature discriminative model (M1)}
\begin{frame}
  \begin{center}
    \structure{Latent-feature discriminative model (M1)}
      \begin{itemize}
        \item We construct a \textbf{deep generative model} that provides a \textbf{robust} set of \textbf{latent features} representation of the data.
        \item Generative Model:
          \begin{align}
            p(z) &= N(z|0,I)\\
            p_{\theta}(x|z) &= f(x;z, \theta)
          \end{align}
        \item Here, $f(x;z,\theta)$ is a suitable likelihood function (e.g. a Gaussian or Bernoulli distribution),
        whose probabilities are formed by a non-linear transformation (deep neural networks), with parameters $\theta$, of a set of latent variables $z$.
        % low dimensional => semi-supervised (aka. small here)
      \end{itemize}
  \end{center}
\end{frame}

\begin{frame}
  \begin{center}
    \structure{Training}
    
\begin{itemize}
      %\item Since exact posterior distribution is intractable (due to the nonlinear, non-conjugate dependencies between the random variables), we use variational inference.
      \item We construct the approximate posterior distribution $q_{\phi}$ as an inference model:
        \begin{align*}
          q_{\phi}(z|x) = N(z|\mu_{\phi}(x), diag(\sigma_{\phi}^2(x))))
        \end{align*}
        Here, $\mu_{\phi}$ and $\sigma_{\phi}$ are MLPs.
      \item Then, we optimize the ELBO:
        \begin{align}
          log p_{\theta}(x) &\geq \E_{q_{\phi}(z|x)}[log p_{\theta}(x|z)] - KL[q_{\phi}(z|x)|| p_{\theta}(z)] 
                            = -J(x)
        \end{align}
      \item Optimization Recap: For the first expectation term, we will use reparametriation + monte carlo samples. For KL, since they are both gaussian distributions, it's analytic. Now, we use the estimated gradients in conjunction with standard stochastic gradient-based optimization methods such as SGD or AdaGrad.
\end{itemize}
  \end{center}
\end{frame}

\begin{frame}
  \begin{center}
    \structure{Application}
    \begin{itemize}
      \item We will use $q_{\phi}(z|x)$ to extract features used for training a classifier such as (transductive) SVM or multinomial regression. 
      \item Using this approach, we can now perform classification in a lower dimensional space since we typically use latent variables whose dimensionality is much less than that of the observations.
      \item This simple approach results in improved performace for SVMs.
    \end{itemize}
  \end{center}
\end{frame}

\section{Generative semi-supervised model (M2)}

\begin{frame}
  \begin{center}
    \structure{Generative semi-supervised model (M2)}
    \begin{itemize}
      \item We propose a probabilistic model that describes the data as being genearted by a latent class variable
      $y$ in addition to a continuous latent variable $z$. 
      \item The data is explained by the generative process:
        \begin{align*}
          p(y) &= Cat(y|\pi)\\
          p(z) &= N(z|0,I)\\
          p_{\theta}(x|y,z) &= f(x;y,z,\theta)\\
        \end{align*}
        Here, $Cat(y,\pi)$ is the multinomial distribution. The class labels $y$ are treated as latent variables if 
        no class label is available and $z$ are additional latent variables.
    \end{itemize}
  \end{center}
\end{frame}


\begin{frame}
  \begin{center}
    \structure{Generative semi-supervised model (M2) - continued}
    \begin{itemize}
      \item The data is explained by the generative process:
        \begin{align*}
          p(y) &= Cat(y|\pi)\\
          p(z) &= N(z|0,I)\\
          p_{\theta}(x|y,z) &= f(x;y,z,\theta)\\
        \end{align*}
      \item Notably, these latent variables are marginally independent, and allow us, in case of digit generation for example, to 
      separate the class specification from the writing style of the digit.
      \item $f(x;y,z,\theta)$, like before, is a suitable likelihood function. 
    \end{itemize}
  \end{center}
\end{frame}
\begin{frame}
  \begin{center}
    \structure{Training}
      \begin{itemize}
        \item We approximate posteriors using inference networks:
          \begin{align}
            q_{\phi}(z,y|x) &= q_{\phi}(z|y,x) q_{\phi}(y|x)\\
            q_{\phi}(z|y,x) &= N(z|\mu_{\phi}(y,x), diag(\sigma_{\phi}^2(x)))\\
            q_{\phi}(y|x) &= Cat(y|\pi_{\phi}(x))\\
          \end{align}
        \item For labeled data, we optimize the ELBO for $p_{\theta}(x,y)$:
          \begin{align}
            log p_{\theta}(x,y) &\geq \E_{q_\phi(z|x,y)}[log p_{\theta}(x|y,z)\\
             &+ log p_{\theta}(y) + log p(z) - log q_{\phi}(z|x,y)]\\
              &= -L(x,y)
          \end{align}
     
        \end{itemize}
  \end{center}
\end{frame}
\begin{frame}
  \begin{center}
    \structure{Training}
      \begin{itemize}
       
        \item For unlabeled data, we treat label as a latent variable, and optimize the ELBO for $p_{\theta}(x)$:
        %TODO: expand

        \begin{align}
          log p_{\theta}(x) &\geq \E_{q_{\phi}(y,z|x)}[log p_{\theta}(x|y,z)\\ &+ log p_{\theta}(y) + log p(z) - log q_{\phi}(y,z|x)]\\
            &= \sum_{y} q_{\phi}(y|x)(-L(x,y)) + H(q_{\phi}(y|x))\\ &= -U(x)
        \end{align}
        \item Thus, the bound on the marginal likelihood for the entire dataset is now:\\
          \begin{align}
            J = \sum_{(x,y) \sim \tilde{p_l}}L(x,y) + \sum_{x \sim \tilde{p_u}}U(x)
          \end{align} 
        \end{itemize}
  \end{center}
\end{frame}

\begin{frame}
  \begin{center}
    \structure{Application}
    \begin{itemize}
      \item We use $q_{\phi}(y|x)$ at test time for predictions (classifier).
      \item In the objective function, the label predictive distribution $q_{\phi}(y|x)$ contributes only to the second term 
      relating to the unlabelled data, which is an undesirable proprety if we wish to use this distribution as a classifier.\\
      % Ideally, all model and variational parameters should learn in all cases. 
      \item To remedy this, we add a classification loss, such that the distribution $q_{\phi}(y|x)$ also learns from labelled data. The extended objective funciton is:
        \begin{align}
          J^{\alpha} = J + \alpha \cdot \E_{\tilde{p_l}(x,y)}[-log q_{\phi}(y|x)]
        \end{align}
        Here, the hyper-parameter $\alpha$ controls the relative weight between generative and purely discriminative learning.
    \end{itemize}
  \end{center}
\end{frame}
\section{Stacked generative semi-supervised model (M1+M2)}

\begin{frame}
  \begin{center}
    \structure{Stacked generative semi-supervised model (M1+M2)}
     \begin{itemize}
        \item We combine previous two models: we first learn a new latent representation $z_1$ with latent variables $z_2$ using the generative model from M1, and subsequently learn a generative semi-supervised model M2, using embeddings from $z_1$ instead of the raw data x.
        \item We can represent it as: 
          \begin{align*}
            p_{\theta}(x,y,z_1,z_2) = p(y)p(z_2)p_{\theta}(z_1|y,z_2) p_{\theta}(x|z_1)
          \end{align*}
      \end{itemize}
  \end{center}
\end{frame}


\section{Results}
\begin{frame}
  \begin{center}

\includegraphics[scale=0.4]{p1.png}
  \end{center}
\end{frame}

\begin{frame}
  \begin{center}

\includegraphics[scale=0.4]{p2.png}

  \end{center}
\end{frame}

\begin{frame}
  \begin{center}

\includegraphics[scale=0.4]{p3.png}

  \end{center}
\end{frame}

\end{document}

